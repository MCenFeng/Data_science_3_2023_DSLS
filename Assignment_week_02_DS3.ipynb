{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Assignment week 02\n",
    "\n",
    "This week's focus is on manifold learning and text clustering. As part of the portfolio assignment, you are required to make a contribution to either the manifold learning case or the text clustering case. There are several options for your contribution, so you can choose the one that aligns with your learning style or interests the most\n",
    "\n",
    "\n",
    "### Manifold learning\n",
    "\n",
    "Study the Tutorial tutorial_manifold_tSNE and the tutorial_manifold_spectral_clustering and the Study_Case_pipeline. Next improve the code by comparing the performance of k-means and spectral clustering. Also compare PCA and t-SNE in the visualization of the result. You can use the pipeline function of scikit-learn and hyperparameter tuning with GridSearchCV. Here's a possible approach:\n",
    "\n",
    "- Load the dataset to be used for the clustering analysis.\n",
    "- Preprocess the dataset as needed (e.g., scale the features, normalize the data, etc.).\n",
    "- Define a pipeline with preprocessing and clustering\n",
    "- use PCA and t-SNE for dimension reduction and visualize the dimensions, use the clusters to color the datapoints\n",
    "- use GridSearchCV to optimize the hyper parameters\n",
    "- Evaluate the performance of the models using a suitable metric\n",
    "- choose the best cluster method and the best visualization method combination\n",
    "\n",
    "Explain choises and evaluate outcome. You can do this assignment in pairs but if you do so mention each others name. Do not forget to reference. If you cannot figure out how to use GridSearchCV and or a pipeline, use your own solution\n",
    "\n",
    "\n",
    "### Text clustering\n",
    "\n",
    "Read, execute and analyse the code in the notebook tutorial_clustering_words. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "a) read the article Clinical Documents Clustering Based on Medication/Symptom Names Using Multi-View Nonnegative Matrix Factorization. you can find the article <a href = 'https://pubmed.ncbi.nlm.nih.gov/26011887/'> here</a>. Explain the similarities of this notebook and the article. Explain in your own words what need to be added to this notebook to reproduce the article. There is no need to code the solution, you can mention in your own words the steps. \n",
    "\n",
    "b) Improve the outcome improving the data preprocessing and the hyper parameter configurations. Explain your choices. Your solution should be a coded solution with comments. Are there any other weighting solutions next to TF-IDF?\n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, you can follow a similar approach to the one in the tutorial_clustering_words notebook. \n",
    "\n",
    "Mind you that you are not allowed to copy code solutions without referencing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold learning and text clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Load and clean the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the dataset used here is from: https://www.kaggle.com/code/ashokrajuyadav/biomedical-text-calssification/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "data_biomedical = pd.read_csv(\"datasets_DS3\\\\alldata_1_for_kaggle.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Solitary plasmacytoma SP of the skull is an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>This study aimed to investigate serum matrix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>7565</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>we report the case of a 24yearold man who pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>7566</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>among synchronous colorectal cancers scrcs rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>7567</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the heterogeneity of cancer cells is generally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>7568</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>\"adipogenesis is the process through which mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>7569</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the periparturient period is one of the most c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0               0  \\\n",
       "0              0  Thyroid_Cancer   \n",
       "1              1  Thyroid_Cancer   \n",
       "2              2  Thyroid_Cancer   \n",
       "3              3  Thyroid_Cancer   \n",
       "4              4  Thyroid_Cancer   \n",
       "...          ...             ...   \n",
       "7565        7565    Colon_Cancer   \n",
       "7566        7566    Colon_Cancer   \n",
       "7567        7567    Colon_Cancer   \n",
       "7568        7568    Colon_Cancer   \n",
       "7569        7569    Colon_Cancer   \n",
       "\n",
       "                                                      a  \n",
       "0     Thyroid surgery in  children in a single insti...  \n",
       "1     \" The adopted strategy was the same as that us...  \n",
       "2     coronary arterybypass grafting thrombosis ï¬b...  \n",
       "3      Solitary plasmacytoma SP of the skull is an u...  \n",
       "4      This study aimed to investigate serum matrix ...  \n",
       "...                                                 ...  \n",
       "7565  we report the case of a 24yearold man who pres...  \n",
       "7566  among synchronous colorectal cancers scrcs rep...  \n",
       "7567  the heterogeneity of cancer cells is generally...  \n",
       "7568  \"adipogenesis is the process through which mes...  \n",
       "7569  the periparturient period is one of the most c...  \n",
       "\n",
       "[7570 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thyroid_Cancer    2810\n",
       "Colon_Cancer      2580\n",
       "Lung_Cancer       2180\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_biomedical.rename(columns={\"Unnamed: 0\": \"index\", \"0\": \"disease\", \"a\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_biomedical.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Solitary plasmacytoma SP of the skull is an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>This study aimed to investigate serum matrix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>we report the case of a 24yearold man who pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>among synchronous colorectal cancers scrcs rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the heterogeneity of cancer cells is generally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>\"adipogenesis is the process through which mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the periparturient period is one of the most c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              disease                                               text\n",
       "index                                                                   \n",
       "0      Thyroid_Cancer  Thyroid surgery in  children in a single insti...\n",
       "1      Thyroid_Cancer  \" The adopted strategy was the same as that us...\n",
       "2      Thyroid_Cancer  coronary arterybypass grafting thrombosis ï¬b...\n",
       "3      Thyroid_Cancer   Solitary plasmacytoma SP of the skull is an u...\n",
       "4      Thyroid_Cancer   This study aimed to investigate serum matrix ...\n",
       "...               ...                                                ...\n",
       "7565     Colon_Cancer  we report the case of a 24yearold man who pres...\n",
       "7566     Colon_Cancer  among synchronous colorectal cancers scrcs rep...\n",
       "7567     Colon_Cancer  the heterogeneity of cancer cells is generally...\n",
       "7568     Colon_Cancer  \"adipogenesis is the process through which mes...\n",
       "7569     Colon_Cancer  the periparturient period is one of the most c...\n",
       "\n",
       "[7570 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def text_cleaning(text):\n",
    "#     # Remove punctuation and convert to lowercase\n",
    "#     text = ''.join([char.lower() if char not in string.punctuation else ' ' for char in text])\n",
    "    \n",
    "#     # Split the text into tokens\n",
    "#     tokens = text.split()\n",
    "    \n",
    "#     # Remove stopwords\n",
    "#     filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "#     return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used for cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = ''.join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = text.split()\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Solitary plasmacytoma SP of the skull is an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>This study aimed to investigate serum matrix ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              disease                                               text\n",
       "index                                                                   \n",
       "0      Thyroid_Cancer  Thyroid surgery in  children in a single insti...\n",
       "1      Thyroid_Cancer  \" The adopted strategy was the same as that us...\n",
       "2      Thyroid_Cancer  coronary arterybypass grafting thrombosis ï¬b...\n",
       "3      Thyroid_Cancer   Solitary plasmacytoma SP of the skull is an u...\n",
       "4      Thyroid_Cancer   This study aimed to investigate serum matrix ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above is taken from: https://www.kaggle.com/code/ashokrajuyadav/biomedical-text-calssification/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step is applied to remove read errors, punctuations, square brackets and symbols, words containing numbers and making text in lowercase for further processing in section B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, \n",
    "    remove punctuation, remove read errors,\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "     # Remove non-alphanumeric characters and extra space\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub('�', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the next part is to use wordnetlemmatizer extract and reduce words to its base root form and then determine whether its a noun or a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thyroid surgery child institution osama ibrahi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strategy year query disjoint citation query qp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arterybypass thrombosis brin brinogen mutation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plasmacytoma sp skull entity proliferation pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>study serum matrix metalloproteinase patient t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      thyroid surgery child institution osama ibrahi...\n",
       "1      strategy year query disjoint citation query qp...\n",
       "2      arterybypass thrombosis brin brinogen mutation...\n",
       "3      plasmacytoma sp skull entity proliferation pla...\n",
       "4      study serum matrix metalloproteinase patient t..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical[\"text\"] = data_biomedical[\"text\"].apply(cleaned)\n",
    "data_nouns = pd.DataFrame(data_biomedical[\"text\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: The Document-Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aac</th>\n",
       "      <th>aag</th>\n",
       "      <th>aat</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>abc</th>\n",
       "      <th>abcam</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>zi</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zj</th>\n",
       "      <th>zl</th>\n",
       "      <th>zone</th>\n",
       "      <th>zou</th>\n",
       "      <th>zscores</th>\n",
       "      <th>zt</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.092709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.014266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026109</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 5302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             aa  aac  aag  aat   ab  abbott  abbreviation  abc     abcam  \\\n",
       "index                                                                      \n",
       "0      0.013158  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "1      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "2      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.020768   \n",
       "3      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "4      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "5      0.014266  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "6      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "7      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "8      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "9      0.000000  0.0  0.0  0.0  0.0     0.0           0.0  0.0  0.000000   \n",
       "\n",
       "       abdomen  ...   zi  zinc   zj   zl      zone  zou  zscores   zt  \\\n",
       "index           ...                                                     \n",
       "0          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "1          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "2          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "3          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "4          0.0  ...  0.0   0.0  0.0  0.0  0.092709  0.0      0.0  0.0   \n",
       "5          0.0  ...  0.0   0.0  0.0  0.0  0.034060  0.0      0.0  0.0   \n",
       "6          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "7          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "8          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "9          0.0  ...  0.0   0.0  0.0  0.0  0.000000  0.0      0.0  0.0   \n",
       "\n",
       "             zy   zz  \n",
       "index                 \n",
       "0      0.000000  0.0  \n",
       "1      0.000000  0.0  \n",
       "2      0.000000  0.0  \n",
       "3      0.000000  0.0  \n",
       "4      0.000000  0.0  \n",
       "5      0.000000  0.0  \n",
       "6      0.000000  0.0  \n",
       "7      0.026109  0.0  \n",
       "8      0.000000  0.0  \n",
       "9      0.000000  0.0  \n",
       "\n",
       "[10 rows x 5302 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# data_nouns and data_biomedical \n",
    "\n",
    "# Create a TF-IDF Vectorizer with specified parameters\n",
    "tv_noun = TfidfVectorizer(stop_words='english', ngram_range=(1, 1), max_df=0.8, min_df=0.01)\n",
    "\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.text)\n",
    "\n",
    "# Create a data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "\n",
    "# Set the index of data_dtm_noun\n",
    "data_dtm_noun.index = data_biomedical.index\n",
    "\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rows: Each row in the DTM corresponds to a document or a piece of text of the dataset.\n",
    "#### The columns: Each column corresponds to a unique term (word) found in the entire corpus (collection of documents). These terms are essentially the features used for analysis. The columns are labeled with the terms they represent, such as 'aa', 'aac', 'aag', 'aat', 'ab', 'abbott', 'abbreviation', and so on\n",
    "#### The values: The values in each cell of the matrix represent the TF-IDF score of the corresponding term in the corresponding document. TF-IDF is a numerical statistic that reflects the importance of a term within a document relative to its importance in the entire corpus. It takes into account both term frequency (how often the term appears in the document) and inverse document frequency (how unique or rare the term is across the entire corpus). Higher TF-IDF scores indicate that a term is more important within a specific document.\n",
    "#### furthermore, the text data, most of the terms are absent or occur infrequently in each document. This leads to a sparse matrix where most values are zero (as indicated by the many 0.0 values in your example). This sparsity is common in text data and is one reason why techniques like TF-IDF are used to represent text data efficiently.\n",
    "#### Stop words have been specified stop_words='english' when creating the TF-IDF vectorizer. This means that common English stop words like \"the,\" \"and,\" \"is,\" etc., have been removed from the text before vectorization. This helps in focusing on more meaningful terms.\n",
    "#### Thus, each row corresponds to a document, each column to a term, and the values represent the importance of each term within each document, with common and unimportant terms filtered out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Run the NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "expression, tumor, mir, tissue, breast, level, proliferation, migration, fig, apoptosis\n",
      "\n",
      "Topic  1\n",
      "disease, risk, group, case, surgery, lung, year, treatment, health, age\n",
      "\n",
      "Topic  2\n",
      "gene, expression, network, pathway, degs, sample, set, model, method, methylation\n",
      "\n",
      "Topic  3\n",
      "mutation, tumor, egfr, lung, alk, pfs, response, gefitinib, trial, dna\n",
      "\n",
      "Topic  4\n",
      "mouse, protein, tumor, virus, infection, response, activity, day, cytokine, activation\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(5)\n",
    "# Learn an NMF model for given Document Term Matrix 'V' \n",
    "# Extract the document-topic matrix 'W'\n",
    "doc_topic = nmf_model.fit_transform(data_dtm_noun)\n",
    "# Extract top words from the topic-term matrix 'H' \n",
    "display_topics(nmf_model, tv_noun.get_feature_names_out(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the given results seen above: \n",
    "#### Topic 0: This topic seems to be related to molecular biology or cellular biology. The top words include \"expression,\" \"tumor,\" \"mir\" (possibly microRNA), \"breast,\" \"proliferation,\" \"migration,\" \"apoptosis,\" and \"fig.\" These terms are often associated with the study of genes, cancer, and cellular processes.\n",
    "#### Topic 1: This topic appears to be related to medical or health-related discussions. The top words include \"disease,\" \"risk,\" \"group,\" \"case,\" \"surgery,\" \"lung,\" \"year,\" \"treatment,\" and \"age.\" It may represent discussions about medical conditions, treatment options, and risk factors.\n",
    "#### Topic 2: This topic seems to be related to bioinformatics or computational biology. The top words include \"gene,\" \"expression,\" \"network,\" \"pathway,\" \"degs\" (possibly differentially expressed genes), \"sample,\" \"set,\" \"model,\" and \"methylation.\" These terms are often used in the context of analyzing gene expression data and biological networks.\n",
    "#### Topic 3: This topic appears to be related to genetics and medical research. The top words include \"mutation,\" \"tumor,\" \"egfr\" (epidermal growth factor receptor), \"lung,\" \"alk\" (anaplastic lymphoma kinase), \"pfs\" (possibly progression-free survival), \"response,\" \"gefitinib\" (a cancer drug), and \"dna.\" This topic might be discussing genetic mutations, clinical trials, and treatment responses in the context of cancer research.\n",
    "#### Topic 4: This topic seems to be related to immunology or virology. The top words include \"mouse,\" \"protein,\" \"tumor,\" \"virus,\" \"infection,\" \"response,\" \"activity,\" \"day,\" \"cytokine,\" and \"activation.\" It may represent discussions about immune responses to viral infections or the role of proteins and cytokines in immune reactions.\n",
    "#### Thus, each document in the corpus is likely a mixture of these topics to varying degrees. For example, a medical research paper might have a higher proportion of words from Topic 3 (genetics) and a lower proportion from Topic 2 (bioinformatics). therefore, topic modeling can be a useful tool for summarizing and understanding the main themes or topics present in a large collection of text documents. It can help in organizing and categorizing documents, making it easier to navigate and analyze a large text corpus."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code used above is taken from: https://github.com/fenna/BFVM23DATASCNC5/blob/main/Tutorials/tutorial_clustering_words.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
