{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Assignment week 02\n",
    "\n",
    "This week's focus is on manifold learning and text clustering. As part of the portfolio assignment, you are required to make a contribution to either the manifold learning case or the text clustering case. There are several options for your contribution, so you can choose the one that aligns with your learning style or interests the most\n",
    "\n",
    "\n",
    "### Manifold learning\n",
    "\n",
    "Study the Tutorial tutorial_manifold_tSNE and the tutorial_manifold_spectral_clustering and the Study_Case_pipeline. Next improve the code by comparing the performance of k-means and spectral clustering. Also compare PCA and t-SNE in the visualization of the result. You can use the pipeline function of scikit-learn and hyperparameter tuning with GridSearchCV. Here's a possible approach:\n",
    "\n",
    "- Load the dataset to be used for the clustering analysis.\n",
    "- Preprocess the dataset as needed (e.g., scale the features, normalize the data, etc.).\n",
    "- Define a pipeline with preprocessing and clustering\n",
    "- use PCA and t-SNE for dimension reduction and visualize the dimensions, use the clusters to color the datapoints\n",
    "- use GridSearchCV to optimize the hyper parameters\n",
    "- Evaluate the performance of the models using a suitable metric\n",
    "- choose the best cluster method and the best visualization method combination\n",
    "\n",
    "Explain choises and evaluate outcome. You can do this assignment in pairs but if you do so mention each others name. Do not forget to reference. If you cannot figure out how to use GridSearchCV and or a pipeline, use your own solution\n",
    "\n",
    "\n",
    "### Text clustering\n",
    "\n",
    "Read, execute and analyse the code in the notebook tutorial_clustering_words. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "a) read the article Clinical Documents Clustering Based on Medication/Symptom Names Using Multi-View Nonnegative Matrix Factorization. you can find the article <a href = 'https://pubmed.ncbi.nlm.nih.gov/26011887/'> here</a>. Explain the similarities of this notebook and the article. Explain in your own words what need to be added to this notebook to reproduce the article. There is no need to code the solution, you can mention in your own words the steps. \n",
    "\n",
    "b) Improve the outcome improving the data preprocessing and the hyper parameter configurations. Explain your choices. Your solution should be a coded solution with comments. Are there any other weighting solutions next to TF-IDF?\n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, you can follow a similar approach to the one in the tutorial_clustering_words notebook. \n",
    "\n",
    "Mind you that you are not allowed to copy code solutions without referencing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold learning and text clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: get and clean the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the dataset used here is from: https://www.kaggle.com/code/ashokrajuyadav/biomedical-text-calssification/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "data_biomedical = pd.read_csv(\"datasets_DS3\\\\alldata_1_for_kaggle.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Solitary plasmacytoma SP of the skull is an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>This study aimed to investigate serum matrix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>7565</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>we report the case of a 24yearold man who pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>7566</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>among synchronous colorectal cancers scrcs rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>7567</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the heterogeneity of cancer cells is generally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>7568</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>\"adipogenesis is the process through which mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>7569</td>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the periparturient period is one of the most c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7570 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0               0  \\\n",
       "0              0  Thyroid_Cancer   \n",
       "1              1  Thyroid_Cancer   \n",
       "2              2  Thyroid_Cancer   \n",
       "3              3  Thyroid_Cancer   \n",
       "4              4  Thyroid_Cancer   \n",
       "...          ...             ...   \n",
       "7565        7565    Colon_Cancer   \n",
       "7566        7566    Colon_Cancer   \n",
       "7567        7567    Colon_Cancer   \n",
       "7568        7568    Colon_Cancer   \n",
       "7569        7569    Colon_Cancer   \n",
       "\n",
       "                                                      a  \n",
       "0     Thyroid surgery in  children in a single insti...  \n",
       "1     \" The adopted strategy was the same as that us...  \n",
       "2     coronary arterybypass grafting thrombosis ï¬b...  \n",
       "3      Solitary plasmacytoma SP of the skull is an u...  \n",
       "4      This study aimed to investigate serum matrix ...  \n",
       "...                                                 ...  \n",
       "7565  we report the case of a 24yearold man who pres...  \n",
       "7566  among synchronous colorectal cancers scrcs rep...  \n",
       "7567  the heterogeneity of cancer cells is generally...  \n",
       "7568  \"adipogenesis is the process through which mes...  \n",
       "7569  the periparturient period is one of the most c...  \n",
       "\n",
       "[7570 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thyroid_Cancer    2810\n",
       "Colon_Cancer      2580\n",
       "Lung_Cancer       2180\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical['0'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_biomedical.rename(columns={\"Unnamed: 0\": \"index\", \"0\": \"disease\", \"a\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_biomedical.set_index(\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Solitary plasmacytoma SP of the skull is an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>This study aimed to investigate serum matrix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>we report the case of a 24yearold man who pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>among synchronous colorectal cancers scrcs rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the heterogeneity of cancer cells is generally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>\"adipogenesis is the process through which mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the periparturient period is one of the most c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              disease                                               text\n",
       "index                                                                   \n",
       "0      Thyroid_Cancer  Thyroid surgery in  children in a single insti...\n",
       "1      Thyroid_Cancer  \" The adopted strategy was the same as that us...\n",
       "2      Thyroid_Cancer  coronary arterybypass grafting thrombosis ï¬b...\n",
       "3      Thyroid_Cancer   Solitary plasmacytoma SP of the skull is an u...\n",
       "4      Thyroid_Cancer   This study aimed to investigate serum matrix ...\n",
       "...               ...                                                ...\n",
       "7565     Colon_Cancer  we report the case of a 24yearold man who pres...\n",
       "7566     Colon_Cancer  among synchronous colorectal cancers scrcs rep...\n",
       "7567     Colon_Cancer  the heterogeneity of cancer cells is generally...\n",
       "7568     Colon_Cancer  \"adipogenesis is the process through which mes...\n",
       "7569     Colon_Cancer  the periparturient period is one of the most c...\n",
       "\n",
       "[7570 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_biomedical.drop('disease', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = ''.join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = text.split()\n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(analyzer = text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_biomedical_counts = vectorizer.fit_transform(data_biomedical['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above is taken from: https://www.kaggle.com/code/ashokrajuyadav/biomedical-text-calssification/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, \n",
    "    remove punctuation, remove read errors,\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub('�', ' ', text)\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thyroid surgery child institution osama ibrahi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strategy year query disjoint citation query qp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arterybypass thrombosis ï¬brin ï¬brinogen mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plasmacytoma sp skull entity proliferation pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>study serum matrix metalloproteinase patient t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      thyroid surgery child institution osama ibrahi...\n",
       "1      strategy year query disjoint citation query qp...\n",
       "2      arterybypass thrombosis ï¬brin ï¬brinogen mu...\n",
       "3      plasmacytoma sp skull entity proliferation pla...\n",
       "4      study serum matrix metalloproteinase patient t..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_biomedical[\"text\"] = data_biomedical[\"text\"].apply(cleaned)\n",
    "data_nouns = pd.DataFrame(data_biomedical[\"text\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: The Document-Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'something', 'seemed', 'themselves', 'since', 'else', 'amongst', 'towards', 'cannot', 'out', 'toward', 'amoungst', 'anyhow', 'further', 'thin', 'how', 'least', 'has', 'when', 'first', 'onto', 'last', 'so', 'besides', 'though', 'until', 'one', 'others', 'full', 'perhaps', 'however', 'part', 'would', 'why', 'two', 'six', 'myself', 'your', 'whereas', 'at', 'might', 'take', 'then', 'latterly', 'they', 'were', 'wherein', 'therefore', 'elsewhere', 'herself', 'below', 'hasnt', 'there', 'empty', 'another', 'anything', 'still', 'most', 'bottom', 'will', 'whole', 'enough', 'yourself', 'up', 'put', 'even', 'from', 'her', 'now', 'per', 'afterwards', 'become', 'than', 'name', 'together', 'already', 'sixty', 'show', 'whose', 'becoming', 'eg', 'nobody', 'serious', 'under', 'co', 'same', 'many', 'itself', 'back', 'describe', 'anyway', 'ltd', 'into', 'noone', 'both', 'between', 'if', 'whoever', 'by', 'mine', 'you', 'mostly', 'thru', 'either', 'their', 'not', 'hence', 'within', 'what', 'made', 'whither', 'seem', 'hers', 'about', 'less', 'sometimes', 'where', 'always', 'beforehand', 'fifteen', 'along', 'cry', 'each', 'thence', 'un', 'find', 'nevertheless', 'none', 'nothing', 'system', 'during', 'everything', 'meanwhile', 'seeming', 'never', 'yours', 'keep', 'a', 'few', 'should', 'with', 'otherwise', 'sometime', 'moreover', 'thereupon', 'we', 'due', 'whenever', 'ourselves', 'couldnt', 'somehow', 'forty', 'done', 'whatever', 'it', 'no', 'only', 'all', 'somewhere', 'seems', 'who', 'third', 'he', 'but', 'five', 'be', 'after', 'go', 'very', 'eleven', 'fill', 'thick', 'whence', 'which', 'she', 'sincere', 'interest', 'that', 'whereby', 'see', 'upon', 'yet', 'formerly', 'too', 'cant', 'the', 're', 'here', 'above', 'whereupon', 'eight', 'get', 'to', 'although', 'for', 'amount', 'before', 'yourselves', 'except', 'again', 'front', 'can', 'beside', 'thereafter', 'former', 'these', 'us', 'etc', 'or', 'inc', 'other', 'had', 'could', 'as', 'hereupon', 'give', 'twenty', 'ever', 'himself', 'its', 'him', 'of', 'nor', 'indeed', 'three', 'some', 'ten', 'among', 'fire', 'more', 'next', 'anyone', 'must', 'throughout', 'was', 'please', 'beyond', 'call', 'hundred', 'hereby', 'my', 'became', 'through', 'via', 'becomes', 'such', 'because', 'whom', 'someone', 'and', 'his', 'neither', 'almost', 'alone', 'thus', 'thereby', 'rather', 'whether', 'behind', 'namely', 'them', 'con', 'ours', 'detail', 'mill', 'therein', 'often', 'latter', 'everyone', 'everywhere', 'been', 'are', 'do', 'an', 'move', 'against', 'may', 'off', 'while', 'nine', 'in', 'me', 'every', 'also', 'without', 'our', 'is', 'well', 'on', 'de', 'twelve', 'wherever', 'across', 'fifty', 'top', 'several', 'down', 'any', 'ie', 'i', 'once', 'being', 'own', 'hereafter', 'side', 'four', 'nowhere', 'those', 'bill', 'herein', 'around', 'am', 'found', 'much', 'over', 'anywhere', 'this', 'have', 'whereafter'}) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m tv_noun \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39mtext\u001b[39m.\u001b[39mENGLISH_STOP_WORDS, ngram_range \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), max_df \u001b[39m=\u001b[39m \u001b[39m.8\u001b[39m, min_df \u001b[39m=\u001b[39m \u001b[39m.01\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data_tv_noun \u001b[39m=\u001b[39m tv_noun\u001b[39m.\u001b[39;49mfit_transform(data_nouns\u001b[39m.\u001b[39;49mtext)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Create data-frame of Doc-Term Matrix with nouns as column names\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data_dtm_noun \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data_tv_noun\u001b[39m.\u001b[39mtoarray(), columns\u001b[39m=\u001b[39mtv_noun\u001b[39m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1368\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1364\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[1;32m-> 1368\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[0;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_ngram_range()\n\u001b[0;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    574\u001b[0m     \u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \n\u001b[0;32m    576\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    582\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[0;32m    583\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    584\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[0;32m    585\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m constraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     98\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'something', 'seemed', 'themselves', 'since', 'else', 'amongst', 'towards', 'cannot', 'out', 'toward', 'amoungst', 'anyhow', 'further', 'thin', 'how', 'least', 'has', 'when', 'first', 'onto', 'last', 'so', 'besides', 'though', 'until', 'one', 'others', 'full', 'perhaps', 'however', 'part', 'would', 'why', 'two', 'six', 'myself', 'your', 'whereas', 'at', 'might', 'take', 'then', 'latterly', 'they', 'were', 'wherein', 'therefore', 'elsewhere', 'herself', 'below', 'hasnt', 'there', 'empty', 'another', 'anything', 'still', 'most', 'bottom', 'will', 'whole', 'enough', 'yourself', 'up', 'put', 'even', 'from', 'her', 'now', 'per', 'afterwards', 'become', 'than', 'name', 'together', 'already', 'sixty', 'show', 'whose', 'becoming', 'eg', 'nobody', 'serious', 'under', 'co', 'same', 'many', 'itself', 'back', 'describe', 'anyway', 'ltd', 'into', 'noone', 'both', 'between', 'if', 'whoever', 'by', 'mine', 'you', 'mostly', 'thru', 'either', 'their', 'not', 'hence', 'within', 'what', 'made', 'whither', 'seem', 'hers', 'about', 'less', 'sometimes', 'where', 'always', 'beforehand', 'fifteen', 'along', 'cry', 'each', 'thence', 'un', 'find', 'nevertheless', 'none', 'nothing', 'system', 'during', 'everything', 'meanwhile', 'seeming', 'never', 'yours', 'keep', 'a', 'few', 'should', 'with', 'otherwise', 'sometime', 'moreover', 'thereupon', 'we', 'due', 'whenever', 'ourselves', 'couldnt', 'somehow', 'forty', 'done', 'whatever', 'it', 'no', 'only', 'all', 'somewhere', 'seems', 'who', 'third', 'he', 'but', 'five', 'be', 'after', 'go', 'very', 'eleven', 'fill', 'thick', 'whence', 'which', 'she', 'sincere', 'interest', 'that', 'whereby', 'see', 'upon', 'yet', 'formerly', 'too', 'cant', 'the', 're', 'here', 'above', 'whereupon', 'eight', 'get', 'to', 'although', 'for', 'amount', 'before', 'yourselves', 'except', 'again', 'front', 'can', 'beside', 'thereafter', 'former', 'these', 'us', 'etc', 'or', 'inc', 'other', 'had', 'could', 'as', 'hereupon', 'give', 'twenty', 'ever', 'himself', 'its', 'him', 'of', 'nor', 'indeed', 'three', 'some', 'ten', 'among', 'fire', 'more', 'next', 'anyone', 'must', 'throughout', 'was', 'please', 'beyond', 'call', 'hundred', 'hereby', 'my', 'became', 'through', 'via', 'becomes', 'such', 'because', 'whom', 'someone', 'and', 'his', 'neither', 'almost', 'alone', 'thus', 'thereby', 'rather', 'whether', 'behind', 'namely', 'them', 'con', 'ours', 'detail', 'mill', 'therein', 'often', 'latter', 'everyone', 'everywhere', 'been', 'are', 'do', 'an', 'move', 'against', 'may', 'off', 'while', 'nine', 'in', 'me', 'every', 'also', 'without', 'our', 'is', 'well', 'on', 'de', 'twelve', 'wherever', 'across', 'fifty', 'top', 'several', 'down', 'any', 'ie', 'i', 'once', 'being', 'own', 'hereafter', 'side', 'four', 'nowhere', 'those', 'bill', 'herein', 'around', 'am', 'found', 'much', 'over', 'anywhere', 'this', 'have', 'whereafter'}) instead."
     ]
    }
   ],
   "source": [
    "# Create a document-term matrix with only nouns\n",
    "# Store TF-IDF Vectorizer\n",
    "tv_noun = TfidfVectorizer(stop_words=text.ENGLISH_STOP_WORDS, ngram_range = (1,1), max_df = .8, min_df = .01)\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.text)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = data_biomedical.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Run the NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(5)\n",
    "# Learn an NMF model for given Document Term Matrix 'V' \n",
    "# Extract the document-topic matrix 'W'\n",
    "doc_topic = nmf_model.fit_transform(data_dtm_noun)\n",
    "# Extract top words from the topic-term matrix 'H' \n",
    "display_topics(nmf_model, tv_noun.get_feature_names_out(), 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code used above is taken from: https://github.com/fenna/BFVM23DATASCNC5/blob/main/Tutorials/tutorial_clustering_words.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
