{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio assignment week 7\n",
    "## 1. Bagging vs Boosting\n",
    "\n",
    "The scikit-learn library provides several options for bagging and boosting. It is possible to create your own boosting model based on a base model. For instance, you can create a tree based bagging model. In addition, scikit-learn provides AdaBoost. For XGBoost it is best to use the xgboost library.\n",
    "\n",
    "Based on the theory in the accompanying notebook, create a bagging, boosting and dummy classifier. Test these classifiers on the breast cancer dataset. Go through the data science pipeline as you've done before:\n",
    "\n",
    "Try to understand the dataset globally.\n",
    "Load the data.\n",
    "Exploratory analysis\n",
    "Preprocess data (skewness, normality, etc.)\n",
    "Modeling (cross-validation and training). (Create several bagging classifiers with different estimators.)\n",
    "Evaluation (Use the evaluation methods as described in the previous lessons. Then compare the different models.)\n",
    "Try to understand why some methods perform better than others. Try different configurations for your bagging and boosting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_brc_bb = pd.read_csv('datasets_DS3\\\\breast-cancer.csv')\n",
    "\n",
    "data_brc_bb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the breast cancer dataset using the column variable diagnosis and checking the variable elements of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    357\n",
       "M    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_brc_bb['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and modifying the data\n",
    "data_brc_bb_clean = data_brc_bb.drop('id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Benign to 0 and Malignant to 1 \n",
    "data_brc_bb_clean['diagnosis'] = data_brc_bb_clean['diagnosis'].map({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations of different elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>diagnosis</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.730029</td>\n",
       "      <td>0.415185</td>\n",
       "      <td>0.742636</td>\n",
       "      <td>0.708984</td>\n",
       "      <td>0.358560</td>\n",
       "      <td>0.596534</td>\n",
       "      <td>0.696360</td>\n",
       "      <td>0.776614</td>\n",
       "      <td>0.330499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776454</td>\n",
       "      <td>0.456903</td>\n",
       "      <td>0.782914</td>\n",
       "      <td>0.733825</td>\n",
       "      <td>0.421465</td>\n",
       "      <td>0.590998</td>\n",
       "      <td>0.659610</td>\n",
       "      <td>0.793566</td>\n",
       "      <td>0.416294</td>\n",
       "      <td>0.323872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_mean</th>\n",
       "      <td>0.730029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323782</td>\n",
       "      <td>0.997855</td>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.170581</td>\n",
       "      <td>0.506124</td>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.822529</td>\n",
       "      <td>0.147741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969539</td>\n",
       "      <td>0.297008</td>\n",
       "      <td>0.965137</td>\n",
       "      <td>0.941082</td>\n",
       "      <td>0.119616</td>\n",
       "      <td>0.413463</td>\n",
       "      <td>0.526911</td>\n",
       "      <td>0.744214</td>\n",
       "      <td>0.163953</td>\n",
       "      <td>0.007066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_mean</th>\n",
       "      <td>0.415185</td>\n",
       "      <td>0.323782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.329533</td>\n",
       "      <td>0.321086</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.302418</td>\n",
       "      <td>0.293464</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352573</td>\n",
       "      <td>0.912045</td>\n",
       "      <td>0.358040</td>\n",
       "      <td>0.343546</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.277830</td>\n",
       "      <td>0.301025</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.119205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_mean</th>\n",
       "      <td>0.742636</td>\n",
       "      <td>0.997855</td>\n",
       "      <td>0.329533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.556936</td>\n",
       "      <td>0.716136</td>\n",
       "      <td>0.850977</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969476</td>\n",
       "      <td>0.303038</td>\n",
       "      <td>0.970387</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>0.455774</td>\n",
       "      <td>0.563879</td>\n",
       "      <td>0.771241</td>\n",
       "      <td>0.189115</td>\n",
       "      <td>0.051019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_mean</th>\n",
       "      <td>0.708984</td>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.321086</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.498502</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.823269</td>\n",
       "      <td>0.151293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962746</td>\n",
       "      <td>0.287489</td>\n",
       "      <td>0.959120</td>\n",
       "      <td>0.959213</td>\n",
       "      <td>0.123523</td>\n",
       "      <td>0.390410</td>\n",
       "      <td>0.512606</td>\n",
       "      <td>0.722017</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>0.003738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_mean</th>\n",
       "      <td>0.358560</td>\n",
       "      <td>0.170581</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.659123</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.553695</td>\n",
       "      <td>0.557775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213120</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.238853</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.805324</td>\n",
       "      <td>0.472468</td>\n",
       "      <td>0.434926</td>\n",
       "      <td>0.503053</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.499316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_mean</th>\n",
       "      <td>0.596534</td>\n",
       "      <td>0.506124</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.556936</td>\n",
       "      <td>0.498502</td>\n",
       "      <td>0.659123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883121</td>\n",
       "      <td>0.831135</td>\n",
       "      <td>0.602641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.248133</td>\n",
       "      <td>0.590210</td>\n",
       "      <td>0.509604</td>\n",
       "      <td>0.565541</td>\n",
       "      <td>0.865809</td>\n",
       "      <td>0.816275</td>\n",
       "      <td>0.815573</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.687382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_mean</th>\n",
       "      <td>0.696360</td>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.302418</td>\n",
       "      <td>0.716136</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.883121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.921391</td>\n",
       "      <td>0.500667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688236</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.729565</td>\n",
       "      <td>0.675987</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.754968</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.409464</td>\n",
       "      <td>0.514930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_mean</th>\n",
       "      <td>0.776614</td>\n",
       "      <td>0.822529</td>\n",
       "      <td>0.293464</td>\n",
       "      <td>0.850977</td>\n",
       "      <td>0.823269</td>\n",
       "      <td>0.553695</td>\n",
       "      <td>0.831135</td>\n",
       "      <td>0.921391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.462497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830318</td>\n",
       "      <td>0.292752</td>\n",
       "      <td>0.855923</td>\n",
       "      <td>0.809630</td>\n",
       "      <td>0.452753</td>\n",
       "      <td>0.667454</td>\n",
       "      <td>0.752399</td>\n",
       "      <td>0.910155</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.368661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_mean</th>\n",
       "      <td>0.330499</td>\n",
       "      <td>0.147741</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>0.151293</td>\n",
       "      <td>0.557775</td>\n",
       "      <td>0.602641</td>\n",
       "      <td>0.500667</td>\n",
       "      <td>0.462497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185728</td>\n",
       "      <td>0.090651</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>0.426675</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.433721</td>\n",
       "      <td>0.430297</td>\n",
       "      <td>0.699826</td>\n",
       "      <td>0.438413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <td>-0.012838</td>\n",
       "      <td>-0.311631</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>-0.261477</td>\n",
       "      <td>-0.283110</td>\n",
       "      <td>0.584792</td>\n",
       "      <td>0.565369</td>\n",
       "      <td>0.336783</td>\n",
       "      <td>0.166917</td>\n",
       "      <td>0.479921</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253691</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>-0.205151</td>\n",
       "      <td>-0.231854</td>\n",
       "      <td>0.504942</td>\n",
       "      <td>0.458798</td>\n",
       "      <td>0.346234</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>0.334019</td>\n",
       "      <td>0.767297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_se</th>\n",
       "      <td>0.567134</td>\n",
       "      <td>0.679090</td>\n",
       "      <td>0.275869</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>0.732562</td>\n",
       "      <td>0.301467</td>\n",
       "      <td>0.497473</td>\n",
       "      <td>0.631925</td>\n",
       "      <td>0.698050</td>\n",
       "      <td>0.303379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715065</td>\n",
       "      <td>0.194799</td>\n",
       "      <td>0.719684</td>\n",
       "      <td>0.751548</td>\n",
       "      <td>0.141919</td>\n",
       "      <td>0.287103</td>\n",
       "      <td>0.380585</td>\n",
       "      <td>0.531062</td>\n",
       "      <td>0.094543</td>\n",
       "      <td>0.049559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_se</th>\n",
       "      <td>-0.008303</td>\n",
       "      <td>-0.097317</td>\n",
       "      <td>0.386358</td>\n",
       "      <td>-0.086761</td>\n",
       "      <td>-0.066280</td>\n",
       "      <td>0.068406</td>\n",
       "      <td>0.046205</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.021480</td>\n",
       "      <td>0.128053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111690</td>\n",
       "      <td>0.409003</td>\n",
       "      <td>-0.102242</td>\n",
       "      <td>-0.083195</td>\n",
       "      <td>-0.073658</td>\n",
       "      <td>-0.092439</td>\n",
       "      <td>-0.068956</td>\n",
       "      <td>-0.119638</td>\n",
       "      <td>-0.128215</td>\n",
       "      <td>-0.045655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_se</th>\n",
       "      <td>0.556141</td>\n",
       "      <td>0.674172</td>\n",
       "      <td>0.281673</td>\n",
       "      <td>0.693135</td>\n",
       "      <td>0.726628</td>\n",
       "      <td>0.296092</td>\n",
       "      <td>0.548905</td>\n",
       "      <td>0.660391</td>\n",
       "      <td>0.710650</td>\n",
       "      <td>0.313893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697201</td>\n",
       "      <td>0.200371</td>\n",
       "      <td>0.721031</td>\n",
       "      <td>0.730713</td>\n",
       "      <td>0.130054</td>\n",
       "      <td>0.341919</td>\n",
       "      <td>0.418899</td>\n",
       "      <td>0.554897</td>\n",
       "      <td>0.109930</td>\n",
       "      <td>0.085433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_se</th>\n",
       "      <td>0.548236</td>\n",
       "      <td>0.735864</td>\n",
       "      <td>0.259845</td>\n",
       "      <td>0.744983</td>\n",
       "      <td>0.800086</td>\n",
       "      <td>0.246552</td>\n",
       "      <td>0.455653</td>\n",
       "      <td>0.617427</td>\n",
       "      <td>0.690299</td>\n",
       "      <td>0.223970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757373</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>0.761213</td>\n",
       "      <td>0.811408</td>\n",
       "      <td>0.125389</td>\n",
       "      <td>0.283257</td>\n",
       "      <td>0.385100</td>\n",
       "      <td>0.538166</td>\n",
       "      <td>0.074126</td>\n",
       "      <td>0.017539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_se</th>\n",
       "      <td>-0.067016</td>\n",
       "      <td>-0.222600</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>-0.202694</td>\n",
       "      <td>-0.166777</td>\n",
       "      <td>0.332375</td>\n",
       "      <td>0.135299</td>\n",
       "      <td>0.098564</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.187321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230691</td>\n",
       "      <td>-0.074743</td>\n",
       "      <td>-0.217304</td>\n",
       "      <td>-0.182195</td>\n",
       "      <td>0.314457</td>\n",
       "      <td>-0.055558</td>\n",
       "      <td>-0.058298</td>\n",
       "      <td>-0.102007</td>\n",
       "      <td>-0.107342</td>\n",
       "      <td>0.101480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_se</th>\n",
       "      <td>0.292999</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.191975</td>\n",
       "      <td>0.250744</td>\n",
       "      <td>0.212583</td>\n",
       "      <td>0.318943</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.670279</td>\n",
       "      <td>0.490424</td>\n",
       "      <td>0.421659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204607</td>\n",
       "      <td>0.143003</td>\n",
       "      <td>0.260516</td>\n",
       "      <td>0.199371</td>\n",
       "      <td>0.227394</td>\n",
       "      <td>0.678780</td>\n",
       "      <td>0.639147</td>\n",
       "      <td>0.483208</td>\n",
       "      <td>0.277878</td>\n",
       "      <td>0.590973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_se</th>\n",
       "      <td>0.253730</td>\n",
       "      <td>0.194204</td>\n",
       "      <td>0.143293</td>\n",
       "      <td>0.228082</td>\n",
       "      <td>0.207660</td>\n",
       "      <td>0.248396</td>\n",
       "      <td>0.570517</td>\n",
       "      <td>0.691270</td>\n",
       "      <td>0.439167</td>\n",
       "      <td>0.342627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186904</td>\n",
       "      <td>0.100241</td>\n",
       "      <td>0.226680</td>\n",
       "      <td>0.188353</td>\n",
       "      <td>0.168481</td>\n",
       "      <td>0.484858</td>\n",
       "      <td>0.662564</td>\n",
       "      <td>0.440472</td>\n",
       "      <td>0.197788</td>\n",
       "      <td>0.439329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_se</th>\n",
       "      <td>0.408042</td>\n",
       "      <td>0.376169</td>\n",
       "      <td>0.163851</td>\n",
       "      <td>0.407217</td>\n",
       "      <td>0.372320</td>\n",
       "      <td>0.380676</td>\n",
       "      <td>0.642262</td>\n",
       "      <td>0.683260</td>\n",
       "      <td>0.615634</td>\n",
       "      <td>0.393298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358127</td>\n",
       "      <td>0.086741</td>\n",
       "      <td>0.394999</td>\n",
       "      <td>0.342271</td>\n",
       "      <td>0.215351</td>\n",
       "      <td>0.452888</td>\n",
       "      <td>0.549592</td>\n",
       "      <td>0.602450</td>\n",
       "      <td>0.143116</td>\n",
       "      <td>0.310655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_se</th>\n",
       "      <td>-0.006522</td>\n",
       "      <td>-0.104321</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>-0.081629</td>\n",
       "      <td>-0.072497</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>0.229977</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.095351</td>\n",
       "      <td>0.449137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128121</td>\n",
       "      <td>-0.077473</td>\n",
       "      <td>-0.103753</td>\n",
       "      <td>-0.110343</td>\n",
       "      <td>-0.012662</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.037119</td>\n",
       "      <td>-0.030413</td>\n",
       "      <td>0.389402</td>\n",
       "      <td>0.078079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_se</th>\n",
       "      <td>0.077972</td>\n",
       "      <td>-0.042641</td>\n",
       "      <td>0.054458</td>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.019887</td>\n",
       "      <td>0.283607</td>\n",
       "      <td>0.507318</td>\n",
       "      <td>0.449301</td>\n",
       "      <td>0.257584</td>\n",
       "      <td>0.331786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037488</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.022736</td>\n",
       "      <td>0.170568</td>\n",
       "      <td>0.390159</td>\n",
       "      <td>0.379975</td>\n",
       "      <td>0.215204</td>\n",
       "      <td>0.111094</td>\n",
       "      <td>0.591328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius_worst</th>\n",
       "      <td>0.776454</td>\n",
       "      <td>0.969539</td>\n",
       "      <td>0.352573</td>\n",
       "      <td>0.969476</td>\n",
       "      <td>0.962746</td>\n",
       "      <td>0.213120</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.688236</td>\n",
       "      <td>0.830318</td>\n",
       "      <td>0.185728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359921</td>\n",
       "      <td>0.993708</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.216574</td>\n",
       "      <td>0.475820</td>\n",
       "      <td>0.573975</td>\n",
       "      <td>0.787424</td>\n",
       "      <td>0.243529</td>\n",
       "      <td>0.093492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture_worst</th>\n",
       "      <td>0.456903</td>\n",
       "      <td>0.297008</td>\n",
       "      <td>0.912045</td>\n",
       "      <td>0.303038</td>\n",
       "      <td>0.287489</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.248133</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.292752</td>\n",
       "      <td>0.090651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359921</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365098</td>\n",
       "      <td>0.345842</td>\n",
       "      <td>0.225429</td>\n",
       "      <td>0.360832</td>\n",
       "      <td>0.368366</td>\n",
       "      <td>0.359755</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.219122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter_worst</th>\n",
       "      <td>0.782914</td>\n",
       "      <td>0.965137</td>\n",
       "      <td>0.358040</td>\n",
       "      <td>0.970387</td>\n",
       "      <td>0.959120</td>\n",
       "      <td>0.238853</td>\n",
       "      <td>0.590210</td>\n",
       "      <td>0.729565</td>\n",
       "      <td>0.855923</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993708</td>\n",
       "      <td>0.365098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.236775</td>\n",
       "      <td>0.529408</td>\n",
       "      <td>0.618344</td>\n",
       "      <td>0.816322</td>\n",
       "      <td>0.269493</td>\n",
       "      <td>0.138957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_worst</th>\n",
       "      <td>0.733825</td>\n",
       "      <td>0.941082</td>\n",
       "      <td>0.343546</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>0.959213</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.509604</td>\n",
       "      <td>0.675987</td>\n",
       "      <td>0.809630</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.345842</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.543331</td>\n",
       "      <td>0.747419</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.079647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness_worst</th>\n",
       "      <td>0.421465</td>\n",
       "      <td>0.119616</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>0.123523</td>\n",
       "      <td>0.805324</td>\n",
       "      <td>0.565541</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.452753</td>\n",
       "      <td>0.426675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216574</td>\n",
       "      <td>0.225429</td>\n",
       "      <td>0.236775</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.568187</td>\n",
       "      <td>0.518523</td>\n",
       "      <td>0.547691</td>\n",
       "      <td>0.493838</td>\n",
       "      <td>0.617624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness_worst</th>\n",
       "      <td>0.590998</td>\n",
       "      <td>0.413463</td>\n",
       "      <td>0.277830</td>\n",
       "      <td>0.455774</td>\n",
       "      <td>0.390410</td>\n",
       "      <td>0.472468</td>\n",
       "      <td>0.865809</td>\n",
       "      <td>0.754968</td>\n",
       "      <td>0.667454</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475820</td>\n",
       "      <td>0.360832</td>\n",
       "      <td>0.529408</td>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.568187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892261</td>\n",
       "      <td>0.801080</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.810455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity_worst</th>\n",
       "      <td>0.659610</td>\n",
       "      <td>0.526911</td>\n",
       "      <td>0.301025</td>\n",
       "      <td>0.563879</td>\n",
       "      <td>0.512606</td>\n",
       "      <td>0.434926</td>\n",
       "      <td>0.816275</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.752399</td>\n",
       "      <td>0.433721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573975</td>\n",
       "      <td>0.368366</td>\n",
       "      <td>0.618344</td>\n",
       "      <td>0.543331</td>\n",
       "      <td>0.518523</td>\n",
       "      <td>0.892261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>0.532520</td>\n",
       "      <td>0.686511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points_worst</th>\n",
       "      <td>0.793566</td>\n",
       "      <td>0.744214</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.771241</td>\n",
       "      <td>0.722017</td>\n",
       "      <td>0.503053</td>\n",
       "      <td>0.815573</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.910155</td>\n",
       "      <td>0.430297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787424</td>\n",
       "      <td>0.359755</td>\n",
       "      <td>0.816322</td>\n",
       "      <td>0.747419</td>\n",
       "      <td>0.547691</td>\n",
       "      <td>0.801080</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502528</td>\n",
       "      <td>0.511114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry_worst</th>\n",
       "      <td>0.416294</td>\n",
       "      <td>0.163953</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.189115</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.409464</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.699826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243529</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.269493</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.493838</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.532520</td>\n",
       "      <td>0.502528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <td>0.323872</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.119205</td>\n",
       "      <td>0.051019</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.499316</td>\n",
       "      <td>0.687382</td>\n",
       "      <td>0.514930</td>\n",
       "      <td>0.368661</td>\n",
       "      <td>0.438413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093492</td>\n",
       "      <td>0.219122</td>\n",
       "      <td>0.138957</td>\n",
       "      <td>0.079647</td>\n",
       "      <td>0.617624</td>\n",
       "      <td>0.810455</td>\n",
       "      <td>0.686511</td>\n",
       "      <td>0.511114</td>\n",
       "      <td>0.537848</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n",
       "diagnosis                 0.000000     0.730029      0.415185        0.742636   \n",
       "radius_mean               0.730029     0.000000      0.323782        0.997855   \n",
       "texture_mean              0.415185     0.323782      0.000000        0.329533   \n",
       "perimeter_mean            0.742636     0.997855      0.329533        0.000000   \n",
       "area_mean                 0.708984     0.987357      0.321086        0.986507   \n",
       "smoothness_mean           0.358560     0.170581     -0.023389        0.207278   \n",
       "compactness_mean          0.596534     0.506124      0.236702        0.556936   \n",
       "concavity_mean            0.696360     0.676764      0.302418        0.716136   \n",
       "concave points_mean       0.776614     0.822529      0.293464        0.850977   \n",
       "symmetry_mean             0.330499     0.147741      0.071401        0.183027   \n",
       "fractal_dimension_mean   -0.012838    -0.311631     -0.076437       -0.261477   \n",
       "radius_se                 0.567134     0.679090      0.275869        0.691765   \n",
       "texture_se               -0.008303    -0.097317      0.386358       -0.086761   \n",
       "perimeter_se              0.556141     0.674172      0.281673        0.693135   \n",
       "area_se                   0.548236     0.735864      0.259845        0.744983   \n",
       "smoothness_se            -0.067016    -0.222600      0.006614       -0.202694   \n",
       "compactness_se            0.292999     0.206000      0.191975        0.250744   \n",
       "concavity_se              0.253730     0.194204      0.143293        0.228082   \n",
       "concave points_se         0.408042     0.376169      0.163851        0.407217   \n",
       "symmetry_se              -0.006522    -0.104321      0.009127       -0.081629   \n",
       "fractal_dimension_se      0.077972    -0.042641      0.054458       -0.005523   \n",
       "radius_worst              0.776454     0.969539      0.352573        0.969476   \n",
       "texture_worst             0.456903     0.297008      0.912045        0.303038   \n",
       "perimeter_worst           0.782914     0.965137      0.358040        0.970387   \n",
       "area_worst                0.733825     0.941082      0.343546        0.941550   \n",
       "smoothness_worst          0.421465     0.119616      0.077503        0.150549   \n",
       "compactness_worst         0.590998     0.413463      0.277830        0.455774   \n",
       "concavity_worst           0.659610     0.526911      0.301025        0.563879   \n",
       "concave points_worst      0.793566     0.744214      0.295316        0.771241   \n",
       "symmetry_worst            0.416294     0.163953      0.105008        0.189115   \n",
       "fractal_dimension_worst   0.323872     0.007066      0.119205        0.051019   \n",
       "\n",
       "                         area_mean  smoothness_mean  compactness_mean  \\\n",
       "diagnosis                 0.708984         0.358560          0.596534   \n",
       "radius_mean               0.987357         0.170581          0.506124   \n",
       "texture_mean              0.321086        -0.023389          0.236702   \n",
       "perimeter_mean            0.986507         0.207278          0.556936   \n",
       "area_mean                 0.000000         0.177028          0.498502   \n",
       "smoothness_mean           0.177028         0.000000          0.659123   \n",
       "compactness_mean          0.498502         0.659123          0.000000   \n",
       "concavity_mean            0.685983         0.521984          0.883121   \n",
       "concave points_mean       0.823269         0.553695          0.831135   \n",
       "symmetry_mean             0.151293         0.557775          0.602641   \n",
       "fractal_dimension_mean   -0.283110         0.584792          0.565369   \n",
       "radius_se                 0.732562         0.301467          0.497473   \n",
       "texture_se               -0.066280         0.068406          0.046205   \n",
       "perimeter_se              0.726628         0.296092          0.548905   \n",
       "area_se                   0.800086         0.246552          0.455653   \n",
       "smoothness_se            -0.166777         0.332375          0.135299   \n",
       "compactness_se            0.212583         0.318943          0.738722   \n",
       "concavity_se              0.207660         0.248396          0.570517   \n",
       "concave points_se         0.372320         0.380676          0.642262   \n",
       "symmetry_se              -0.072497         0.200774          0.229977   \n",
       "fractal_dimension_se     -0.019887         0.283607          0.507318   \n",
       "radius_worst              0.962746         0.213120          0.535315   \n",
       "texture_worst             0.287489         0.036072          0.248133   \n",
       "perimeter_worst           0.959120         0.238853          0.590210   \n",
       "area_worst                0.959213         0.206718          0.509604   \n",
       "smoothness_worst          0.123523         0.805324          0.565541   \n",
       "compactness_worst         0.390410         0.472468          0.865809   \n",
       "concavity_worst           0.512606         0.434926          0.816275   \n",
       "concave points_worst      0.722017         0.503053          0.815573   \n",
       "symmetry_worst            0.143570         0.394309          0.510223   \n",
       "fractal_dimension_worst   0.003738         0.499316          0.687382   \n",
       "\n",
       "                         concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "diagnosis                      0.696360             0.776614       0.330499   \n",
       "radius_mean                    0.676764             0.822529       0.147741   \n",
       "texture_mean                   0.302418             0.293464       0.071401   \n",
       "perimeter_mean                 0.716136             0.850977       0.183027   \n",
       "area_mean                      0.685983             0.823269       0.151293   \n",
       "smoothness_mean                0.521984             0.553695       0.557775   \n",
       "compactness_mean               0.883121             0.831135       0.602641   \n",
       "concavity_mean                 0.000000             0.921391       0.500667   \n",
       "concave points_mean            0.921391             0.000000       0.462497   \n",
       "symmetry_mean                  0.500667             0.462497       0.000000   \n",
       "fractal_dimension_mean         0.336783             0.166917       0.479921   \n",
       "radius_se                      0.631925             0.698050       0.303379   \n",
       "texture_se                     0.076218             0.021480       0.128053   \n",
       "perimeter_se                   0.660391             0.710650       0.313893   \n",
       "area_se                        0.617427             0.690299       0.223970   \n",
       "smoothness_se                  0.098564             0.027653       0.187321   \n",
       "compactness_se                 0.670279             0.490424       0.421659   \n",
       "concavity_se                   0.691270             0.439167       0.342627   \n",
       "concave points_se              0.683260             0.615634       0.393298   \n",
       "symmetry_se                    0.178009             0.095351       0.449137   \n",
       "fractal_dimension_se           0.449301             0.257584       0.331786   \n",
       "radius_worst                   0.688236             0.830318       0.185728   \n",
       "texture_worst                  0.299879             0.292752       0.090651   \n",
       "perimeter_worst                0.729565             0.855923       0.219169   \n",
       "area_worst                     0.675987             0.809630       0.177193   \n",
       "smoothness_worst               0.448822             0.452753       0.426675   \n",
       "compactness_worst              0.754968             0.667454       0.473200   \n",
       "concavity_worst                0.884103             0.752399       0.433721   \n",
       "concave points_worst           0.861323             0.910155       0.430297   \n",
       "symmetry_worst                 0.409464             0.375744       0.699826   \n",
       "fractal_dimension_worst        0.514930             0.368661       0.438413   \n",
       "\n",
       "                         ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "diagnosis                ...      0.776454       0.456903         0.782914   \n",
       "radius_mean              ...      0.969539       0.297008         0.965137   \n",
       "texture_mean             ...      0.352573       0.912045         0.358040   \n",
       "perimeter_mean           ...      0.969476       0.303038         0.970387   \n",
       "area_mean                ...      0.962746       0.287489         0.959120   \n",
       "smoothness_mean          ...      0.213120       0.036072         0.238853   \n",
       "compactness_mean         ...      0.535315       0.248133         0.590210   \n",
       "concavity_mean           ...      0.688236       0.299879         0.729565   \n",
       "concave points_mean      ...      0.830318       0.292752         0.855923   \n",
       "symmetry_mean            ...      0.185728       0.090651         0.219169   \n",
       "fractal_dimension_mean   ...     -0.253691      -0.051269        -0.205151   \n",
       "radius_se                ...      0.715065       0.194799         0.719684   \n",
       "texture_se               ...     -0.111690       0.409003        -0.102242   \n",
       "perimeter_se             ...      0.697201       0.200371         0.721031   \n",
       "area_se                  ...      0.757373       0.196497         0.761213   \n",
       "smoothness_se            ...     -0.230691      -0.074743        -0.217304   \n",
       "compactness_se           ...      0.204607       0.143003         0.260516   \n",
       "concavity_se             ...      0.186904       0.100241         0.226680   \n",
       "concave points_se        ...      0.358127       0.086741         0.394999   \n",
       "symmetry_se              ...     -0.128121      -0.077473        -0.103753   \n",
       "fractal_dimension_se     ...     -0.037488      -0.003195        -0.001000   \n",
       "radius_worst             ...      0.000000       0.359921         0.993708   \n",
       "texture_worst            ...      0.359921       0.000000         0.365098   \n",
       "perimeter_worst          ...      0.993708       0.365098         0.000000   \n",
       "area_worst               ...      0.984015       0.345842         0.977578   \n",
       "smoothness_worst         ...      0.216574       0.225429         0.236775   \n",
       "compactness_worst        ...      0.475820       0.360832         0.529408   \n",
       "concavity_worst          ...      0.573975       0.368366         0.618344   \n",
       "concave points_worst     ...      0.787424       0.359755         0.816322   \n",
       "symmetry_worst           ...      0.243529       0.233027         0.269493   \n",
       "fractal_dimension_worst  ...      0.093492       0.219122         0.138957   \n",
       "\n",
       "                         area_worst  smoothness_worst  compactness_worst  \\\n",
       "diagnosis                  0.733825          0.421465           0.590998   \n",
       "radius_mean                0.941082          0.119616           0.413463   \n",
       "texture_mean               0.343546          0.077503           0.277830   \n",
       "perimeter_mean             0.941550          0.150549           0.455774   \n",
       "area_mean                  0.959213          0.123523           0.390410   \n",
       "smoothness_mean            0.206718          0.805324           0.472468   \n",
       "compactness_mean           0.509604          0.565541           0.865809   \n",
       "concavity_mean             0.675987          0.448822           0.754968   \n",
       "concave points_mean        0.809630          0.452753           0.667454   \n",
       "symmetry_mean              0.177193          0.426675           0.473200   \n",
       "fractal_dimension_mean    -0.231854          0.504942           0.458798   \n",
       "radius_se                  0.751548          0.141919           0.287103   \n",
       "texture_se                -0.083195         -0.073658          -0.092439   \n",
       "perimeter_se               0.730713          0.130054           0.341919   \n",
       "area_se                    0.811408          0.125389           0.283257   \n",
       "smoothness_se             -0.182195          0.314457          -0.055558   \n",
       "compactness_se             0.199371          0.227394           0.678780   \n",
       "concavity_se               0.188353          0.168481           0.484858   \n",
       "concave points_se          0.342271          0.215351           0.452888   \n",
       "symmetry_se               -0.110343         -0.012662           0.060255   \n",
       "fractal_dimension_se      -0.022736          0.170568           0.390159   \n",
       "radius_worst               0.984015          0.216574           0.475820   \n",
       "texture_worst              0.345842          0.225429           0.360832   \n",
       "perimeter_worst            0.977578          0.236775           0.529408   \n",
       "area_worst                 0.000000          0.209145           0.438296   \n",
       "smoothness_worst           0.209145          0.000000           0.568187   \n",
       "compactness_worst          0.438296          0.568187           0.000000   \n",
       "concavity_worst            0.543331          0.518523           0.892261   \n",
       "concave points_worst       0.747419          0.547691           0.801080   \n",
       "symmetry_worst             0.209146          0.493838           0.614441   \n",
       "fractal_dimension_worst    0.079647          0.617624           0.810455   \n",
       "\n",
       "                         concavity_worst  concave points_worst  \\\n",
       "diagnosis                       0.659610              0.793566   \n",
       "radius_mean                     0.526911              0.744214   \n",
       "texture_mean                    0.301025              0.295316   \n",
       "perimeter_mean                  0.563879              0.771241   \n",
       "area_mean                       0.512606              0.722017   \n",
       "smoothness_mean                 0.434926              0.503053   \n",
       "compactness_mean                0.816275              0.815573   \n",
       "concavity_mean                  0.884103              0.861323   \n",
       "concave points_mean             0.752399              0.910155   \n",
       "symmetry_mean                   0.433721              0.430297   \n",
       "fractal_dimension_mean          0.346234              0.175325   \n",
       "radius_se                       0.380585              0.531062   \n",
       "texture_se                     -0.068956             -0.119638   \n",
       "perimeter_se                    0.418899              0.554897   \n",
       "area_se                         0.385100              0.538166   \n",
       "smoothness_se                  -0.058298             -0.102007   \n",
       "compactness_se                  0.639147              0.483208   \n",
       "concavity_se                    0.662564              0.440472   \n",
       "concave points_se               0.549592              0.602450   \n",
       "symmetry_se                     0.037119             -0.030413   \n",
       "fractal_dimension_se            0.379975              0.215204   \n",
       "radius_worst                    0.573975              0.787424   \n",
       "texture_worst                   0.368366              0.359755   \n",
       "perimeter_worst                 0.618344              0.816322   \n",
       "area_worst                      0.543331              0.747419   \n",
       "smoothness_worst                0.518523              0.547691   \n",
       "compactness_worst               0.892261              0.801080   \n",
       "concavity_worst                 0.000000              0.855434   \n",
       "concave points_worst            0.855434              0.000000   \n",
       "symmetry_worst                  0.532520              0.502528   \n",
       "fractal_dimension_worst         0.686511              0.511114   \n",
       "\n",
       "                         symmetry_worst  fractal_dimension_worst  \n",
       "diagnosis                      0.416294                 0.323872  \n",
       "radius_mean                    0.163953                 0.007066  \n",
       "texture_mean                   0.105008                 0.119205  \n",
       "perimeter_mean                 0.189115                 0.051019  \n",
       "area_mean                      0.143570                 0.003738  \n",
       "smoothness_mean                0.394309                 0.499316  \n",
       "compactness_mean               0.510223                 0.687382  \n",
       "concavity_mean                 0.409464                 0.514930  \n",
       "concave points_mean            0.375744                 0.368661  \n",
       "symmetry_mean                  0.699826                 0.438413  \n",
       "fractal_dimension_mean         0.334019                 0.767297  \n",
       "radius_se                      0.094543                 0.049559  \n",
       "texture_se                    -0.128215                -0.045655  \n",
       "perimeter_se                   0.109930                 0.085433  \n",
       "area_se                        0.074126                 0.017539  \n",
       "smoothness_se                 -0.107342                 0.101480  \n",
       "compactness_se                 0.277878                 0.590973  \n",
       "concavity_se                   0.197788                 0.439329  \n",
       "concave points_se              0.143116                 0.310655  \n",
       "symmetry_se                    0.389402                 0.078079  \n",
       "fractal_dimension_se           0.111094                 0.591328  \n",
       "radius_worst                   0.243529                 0.093492  \n",
       "texture_worst                  0.233027                 0.219122  \n",
       "perimeter_worst                0.269493                 0.138957  \n",
       "area_worst                     0.209146                 0.079647  \n",
       "smoothness_worst               0.493838                 0.617624  \n",
       "compactness_worst              0.614441                 0.810455  \n",
       "concavity_worst                0.532520                 0.686511  \n",
       "concave points_worst           0.502528                 0.511114  \n",
       "symmetry_worst                 0.000000                 0.537848  \n",
       "fractal_dimension_worst        0.537848                 0.000000  \n",
       "\n",
       "[31 rows x 31 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_mat = data_brc_bb_clean.corr()\n",
    "# Strip out the diagonal values for the next step\n",
    "for x in range(len(data_brc_bb_clean.columns)):\n",
    "    corr_mat.iloc[x,x] = 0.0\n",
    "    \n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                     concave points_worst\n",
       "radius_mean                         perimeter_mean\n",
       "texture_mean                         texture_worst\n",
       "perimeter_mean                         radius_mean\n",
       "area_mean                              radius_mean\n",
       "smoothness_mean                   smoothness_worst\n",
       "compactness_mean                    concavity_mean\n",
       "concavity_mean                 concave points_mean\n",
       "concave points_mean                 concavity_mean\n",
       "symmetry_mean                       symmetry_worst\n",
       "fractal_dimension_mean     fractal_dimension_worst\n",
       "radius_se                             perimeter_se\n",
       "texture_se                             symmetry_se\n",
       "perimeter_se                             radius_se\n",
       "area_se                                  radius_se\n",
       "smoothness_se                 fractal_dimension_se\n",
       "compactness_se                fractal_dimension_se\n",
       "concavity_se                        compactness_se\n",
       "concave points_se                     concavity_se\n",
       "symmetry_se                          symmetry_mean\n",
       "fractal_dimension_se                compactness_se\n",
       "radius_worst                       perimeter_worst\n",
       "texture_worst                         texture_mean\n",
       "perimeter_worst                       radius_worst\n",
       "area_worst                            radius_worst\n",
       "smoothness_worst                   smoothness_mean\n",
       "compactness_worst                  concavity_worst\n",
       "concavity_worst                  compactness_worst\n",
       "concave points_worst           concave points_mean\n",
       "symmetry_worst                       symmetry_mean\n",
       "fractal_dimension_worst          compactness_worst\n",
       "dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see which features are highly correlated\n",
    "# Pairwise maximal correlations\n",
    "corr_mat.abs().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                  0.793566\n",
       "radius_mean                0.997855\n",
       "texture_mean               0.912045\n",
       "perimeter_mean             0.997855\n",
       "area_mean                  0.987357\n",
       "smoothness_mean            0.805324\n",
       "compactness_mean           0.883121\n",
       "concavity_mean             0.921391\n",
       "concave points_mean        0.921391\n",
       "symmetry_mean              0.699826\n",
       "fractal_dimension_mean     0.767297\n",
       "radius_se                  0.972794\n",
       "texture_se                 0.411621\n",
       "perimeter_se               0.972794\n",
       "area_se                    0.951830\n",
       "smoothness_se              0.427374\n",
       "compactness_se             0.803269\n",
       "concavity_se               0.801268\n",
       "concave points_se          0.771804\n",
       "symmetry_se                0.449137\n",
       "fractal_dimension_se       0.803269\n",
       "radius_worst               0.993708\n",
       "texture_worst              0.912045\n",
       "perimeter_worst            0.993708\n",
       "area_worst                 0.984015\n",
       "smoothness_worst           0.805324\n",
       "compactness_worst          0.892261\n",
       "concavity_worst            0.892261\n",
       "concave points_worst       0.910155\n",
       "symmetry_worst             0.699826\n",
       "fractal_dimension_worst    0.810455\n",
       "dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how much are they correlated? Can we eliminate certain features based on high correlations\n",
    "corr_mat.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "area_se                    5.447186\n",
       "concavity_se               5.110463\n",
       "fractal_dimension_se       3.923969\n",
       "perimeter_se               3.443615\n",
       "radius_se                  3.088612\n",
       "smoothness_se              2.314450\n",
       "symmetry_se                2.195133\n",
       "compactness_se             1.902221\n",
       "area_worst                 1.859373\n",
       "fractal_dimension_worst    1.662579\n",
       "texture_se                 1.646444\n",
       "area_mean                  1.645732\n",
       "compactness_worst          1.473555\n",
       "concave points_se          1.444678\n",
       "symmetry_worst             1.433928\n",
       "concavity_mean             1.401180\n",
       "fractal_dimension_mean     1.304489\n",
       "compactness_mean           1.190123\n",
       "concave points_mean        1.171180\n",
       "concavity_worst            1.150237\n",
       "perimeter_worst            1.128164\n",
       "radius_worst               1.103115\n",
       "perimeter_mean             0.990650\n",
       "radius_mean                0.942380\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .skew 0: no skew, + right skew, - left skew, look for above .75 \n",
    "skew_columns = (data_brc_bb_clean\n",
    "                .skew()\n",
    "                .sort_values(ascending=False))\n",
    "\n",
    "skew_columns = skew_columns.loc[skew_columns > 0.75]\n",
    "skew_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform log transform on skewed columns\n",
    "for col in skew_columns.index.tolist():\n",
    "    data_brc_bb_clean[col] = np.log1p(data_brc_bb_clean[col])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Bagging vs Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The implementtation of the Bagging and Boosting for the dataset is to reduce the errors on the prediction of Element column 'Diagnosis' based on two variables which are 'Malignant' and 'Benign'. To whether these ensemble methods could reduce the overfitting and increase the accuracy, while reducing the bias of the results based on the column 'Diagnosis'. Several combination of weak learners of the bagging methods such as Bagging classifier, Random Forest classifier and Extra Trees classifier were used to produce a model with lower variance and then different estimators were used, such as, Decision Tree, K-neighbors and Support Vector Machine (SVC) to check the improvment of the Bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_brc_bb_clean.drop('diagnosis', axis=1)\n",
    "y= data_brc_bb_clean['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (455, 30)\n",
      "X_test shape: (114, 30)\n",
      "y_train shape: (455,)\n",
      "y_test shape: (114,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\",X_train.shape)\n",
    "print(\"X_test shape:\",X_test.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "print(\"y_test shape:\",y_test.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store results of models\n",
    "result_dict_train = {}\n",
    "result_dict_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "def evaluate(model, X_train, X_test, y_train, y_test):\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "    print(\"TRAINING RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_train, y_train_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "\n",
    "    print(\"TESTING RESULTS: \\n===============================\")\n",
    "    clf_report = pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))\n",
    "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_test_pred)}\")\n",
    "    print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[286   0]\n",
      " [  0 169]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    286.0  169.0       1.0      455.0         455.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[69  2]\n",
      " [ 3 40]]\n",
      "ACCURACY SCORE:\n",
      "0.9561\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.958333   0.952381   0.95614    0.955357      0.956088\n",
      "recall      0.971831   0.930233   0.95614    0.951032      0.956140\n",
      "f1-score    0.965035   0.941176   0.95614    0.953106      0.956036\n",
      "support    71.000000  43.000000   0.95614  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Bagging using the DecisionTree classifier\n",
    "bagging_tree = DecisionTreeClassifier()\n",
    "bagging_clf = BaggingClassifier(base_estimator=bagging_tree, n_estimators=1500, random_state=42)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate(bagging_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[276  10]\n",
      " [ 21 148]]\n",
      "ACCURACY SCORE:\n",
      "0.9319\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.929293    0.936709  0.931868    0.933001      0.932047\n",
      "recall       0.965035    0.875740  0.931868    0.920387      0.931868\n",
      "f1-score     0.946827    0.905199  0.931868    0.926013      0.931365\n",
      "support    286.000000  169.000000  0.931868  455.000000    455.000000\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[67  4]\n",
      " [ 7 36]]\n",
      "ACCURACY SCORE:\n",
      "0.9035\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.905405   0.900000  0.903509    0.902703      0.903367\n",
      "recall      0.943662   0.837209  0.903509    0.890436      0.903509\n",
      "f1-score    0.924138   0.867470  0.903509    0.895804      0.902763\n",
      "support    71.000000  43.000000  0.903509  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "# Bagging using the k-nearest neighbor classifier\n",
    "bagging_knn =KNeighborsClassifier()\n",
    "bagging_clf_2 = BaggingClassifier(base_estimator=bagging_knn, n_estimators=1500, random_state=42)\n",
    "bagging_clf_2.fit(X_train, y_train)\n",
    "\n",
    "evaluate(bagging_clf_2, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[257  29]\n",
      " [ 44 125]]\n",
      "ACCURACY SCORE:\n",
      "0.8396\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.853821    0.811688   0.83956    0.832754      0.838171\n",
      "recall       0.898601    0.739645   0.83956    0.819123      0.839560\n",
      "f1-score     0.875639    0.773994   0.83956    0.824816      0.837885\n",
      "support    286.000000  169.000000   0.83956  455.000000    455.000000\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[60 11]\n",
      " [ 9 34]]\n",
      "ACCURACY SCORE:\n",
      "0.8246\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.869565   0.755556  0.824561    0.812560      0.826562\n",
      "recall      0.845070   0.790698  0.824561    0.817884      0.824561\n",
      "f1-score    0.857143   0.772727  0.824561    0.814935      0.825302\n",
      "support    71.000000  43.000000  0.824561  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "# Bagging using the Support Vector Machine classifier\n",
    "bagging_svm = SVC()\n",
    "bagging_clf_3 = BaggingClassifier(base_estimator=bagging_svm, n_estimators=1500, random_state=42)\n",
    "bagging_clf_3.fit(X_train, y_train)\n",
    "\n",
    "evaluate(bagging_clf_3, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Bagging Classifier tree'] = {\n",
    "        'Train': accuracy_score(y_train, bagging_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, bagging_clf.predict(X_test)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Bagging Classifier knn '] = {\n",
    "        'Train': accuracy_score(y_train, bagging_clf_2.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, bagging_clf_2.predict(X_test)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Bagging Classifier svm'] = {\n",
    "        'Train': accuracy_score(y_train, bagging_clf_3.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, bagging_clf_3.predict(X_test)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[286   0]\n",
      " [  0 169]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    286.0  169.0       1.0      455.0         455.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[70  1]\n",
      " [ 3 40]]\n",
      "ACCURACY SCORE:\n",
      "0.9649\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.958904   0.975610  0.964912    0.967257      0.965205\n",
      "recall      0.985915   0.930233  0.964912    0.958074      0.964912\n",
      "f1-score    0.972222   0.952381  0.964912    0.962302      0.964738\n",
      "support    71.000000  43.000000  0.964912  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_estimators=1000)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "evaluate(rf_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Random Forest'] = {\n",
    "        'Train': accuracy_score(y_train, rf_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, rf_clf.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[286   0]\n",
      " [  0 169]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    286.0  169.0       1.0      455.0         455.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[70  1]\n",
      " [ 2 41]]\n",
      "ACCURACY SCORE:\n",
      "0.9737\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.972222   0.976190  0.973684    0.974206      0.973719\n",
      "recall      0.985915   0.953488  0.973684    0.969702      0.973684\n",
      "f1-score    0.979021   0.964706  0.973684    0.971863      0.973621\n",
      "support    71.000000  43.000000  0.973684  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ex_tree_clf = ExtraTreesClassifier(n_estimators=1000, max_features=7, random_state=42)\n",
    "ex_tree_clf.fit(X_train, y_train)\n",
    "evaluate(ex_tree_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Extra Tree'] = {\n",
    "        'Train': accuracy_score(y_train, ex_tree_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, ex_tree_clf.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the boosting methods, Adaboost and Gradient boosting are implemented to reduce the bias the of model based on the Element 'Diagnosis'. For each subsequent learner it should improve the errors of the previous learners of the Bagging methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[286   0]\n",
      " [  0 169]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    286.0  169.0       1.0      455.0         455.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[71  0]\n",
      " [ 3 40]]\n",
      "ACCURACY SCORE:\n",
      "0.9737\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.959459   1.000000  0.973684    0.979730      0.974751\n",
      "recall      1.000000   0.930233  0.973684    0.965116      0.973684\n",
      "f1-score    0.979310   0.963855  0.973684    0.971583      0.973481\n",
      "support    71.000000  43.000000  0.973684  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_boost_clf = AdaBoostClassifier(n_estimators=30)\n",
    "ada_boost_clf.fit(X_train, y_train)\n",
    "evaluate(ada_boost_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['AdaBoost'] = {\n",
    "        'Train': accuracy_score(y_train, ada_boost_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, ada_boost_clf.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[286   0]\n",
      " [  0 169]]\n",
      "ACCURACY SCORE:\n",
      "1.0000\n",
      "CLASSIFICATION REPORT:\n",
      "               0      1  accuracy  macro avg  weighted avg\n",
      "precision    1.0    1.0       1.0        1.0           1.0\n",
      "recall       1.0    1.0       1.0        1.0           1.0\n",
      "f1-score     1.0    1.0       1.0        1.0           1.0\n",
      "support    286.0  169.0       1.0      455.0         455.0\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[69  2]\n",
      " [ 3 40]]\n",
      "ACCURACY SCORE:\n",
      "0.9561\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.958333   0.952381   0.95614    0.955357      0.956088\n",
      "recall      0.971831   0.930233   0.95614    0.951032      0.956140\n",
      "f1-score    0.965035   0.941176   0.95614    0.953106      0.956036\n",
      "support    71.000000  43.000000   0.95614  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "grad_boost_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "grad_boost_clf.fit(X_train, y_train)\n",
    "evaluate(grad_boost_clf, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Gradient Boosting'] = {\n",
    "        'Train': accuracy_score(y_train, grad_boost_clf.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, grad_boost_clf.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The stacking methods are used to improve the model's accuracy of the previous combined learners of the Bagging and Boosting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINIG RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[283   3]\n",
      " [ 11 158]]\n",
      "ACCURACY SCORE:\n",
      "0.9692\n",
      "CLASSIFICATION REPORT:\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.962585    0.981366  0.969231    0.971976      0.969561\n",
      "recall       0.989510    0.934911  0.969231    0.962211      0.969231\n",
      "f1-score     0.975862    0.957576  0.969231    0.966719      0.969070\n",
      "support    286.000000  169.000000  0.969231  455.000000    455.000000\n",
      "TESTING RESULTS: \n",
      "===============================\n",
      "CONFUSION MATRIX:\n",
      "[[70  1]\n",
      " [ 4 39]]\n",
      "ACCURACY SCORE:\n",
      "0.9561\n",
      "CLASSIFICATION REPORT:\n",
      "                   0          1  accuracy   macro avg  weighted avg\n",
      "precision   0.945946   0.975000   0.95614    0.960473      0.956905\n",
      "recall      0.985915   0.906977   0.95614    0.946446      0.956140\n",
      "f1-score    0.965517   0.939759   0.95614    0.952638      0.955801\n",
      "support    71.000000  43.000000   0.95614  114.000000    114.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "estimators = []\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "estimators.append(('Logistic', log_reg))\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "estimators.append(('Tree', tree))\n",
    "\n",
    "svm_clf = SVC(gamma='scale')\n",
    "estimators.append(('SVM', svm_clf))\n",
    "\n",
    "voting = VotingClassifier(estimators=estimators)\n",
    "voting.fit(X_train, y_train)\n",
    "\n",
    "evaluate(voting, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['Voting'] = {\n",
    "        'Train': accuracy_score(y_train, voting.predict(X_train)),\n",
    "        'Test': accuracy_score(y_test, voting.predict(X_test)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "scores_df.plot(kind='barh', figsize=(15, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the above code was taken from the Kaggle(Ensemble ML Algorithms : Bagging, Boosting, Voting) https://www.kaggle.com/code/faressayah/ensemble-ml-algorithms-bagging-boosting-voting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the models put together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning models used here are: Bagging, Random Forest, Extra tree, Adaboost and Gradient Boosting. From the bagging classifier three different estimators are used to determine its performance accuracy. It is seen that the bagging classifier with the estimator 'Tree' has a higher overall accuracy score compared to 'support vector machine' and 'K-nearest neighbor' estimators. \n",
    "As seen from the from the graph given above, that adaboost has higher overall accuracy score compared to gradient boosting, same with extra tree compared to random forest. These overall accuracy scores are seen from the test results\n",
    "The voting classiefier results tells you that it trained all the base models to improve the performance of the this machine learning model. The overall improvement to the model from voting is very good which is above the 0.9.\n",
    "Overall, the model performs well on both the training and testing data, with high accuracy scores. However, there is a slight drop in performance on the testing data compared to the training data, which is expected. The classification report provides more detailed information about the model's performance for each class, including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
